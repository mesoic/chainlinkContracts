Flux Monitor Implementation: The objective of this exercise is to create a minimal working example of the chainlink network by launhing a cluster of nodes (n=3) and aggregating results of the API requests in in a flux aggregator contract. The flux monitor contract to be used is: 

	AccessControlledAggregator.sol

In order to achieve this, an existing node infrastructure will be used. The first step is to update the node infrastructure to the most recent version of chainlink (v0.10.1), and to verify the functionality of the node via liveness jobs(cron) which request the ETHUSD price, and via requests through an existing consumer contract which requests the LINKUSD price through the cryptocompare external adapter. All infrastructure is depolyed on the Ropsten testnet. 

Once the node is verified as functional, the existing infrastrustucture is expanded to the case of multiple nodes. For simplicity, all nodes will be deployed as separate docker containers on a single EC2 instance. 


A) Update the basics: 

	Update Linux AMI:
		sudo yum update (linux AMI)
		docker pull smartcontract/chainlink:0.10.1
		docker pull fiews/cl-eth-failover

	Get new fiews API key: 	

B) Start the node with updated chainlink version and patch database: 

	Use existing AWS architecture 
	Restart Postgres Database 

	Note: The node is configured to report to cloudWatch logs. Errors were encountered when starting the node

		1) 	{ "level": "error", "ts": 1614708904.2861276, "caller": "logger/default.go:139", "msg": "unable to lock ORM: dial tcp: lookup chainlink-postgres.cpkt4axduud7.eu-central-1.rds.amazonaws.com on 172.31.0.2:53: no such host", "stacktrace": }

			The reason for this was an incorrect endpioint configuration when starting the DB in the AWS console. The endpoint must be consistent with appears in the chainlink .env file.

		2) { "level": "fatal", "ts": 1614709795.4249516, "caller": "store/store.go:70", "msg": "Unable to initialize ORM: pq: invalid input value for enum run_status: \"pending_confirmations\"\nerror running }

			Recall from last time (restarting node - cold start), that we need to interact with the database in order to recover this error. 

				./scripts/chainlink-postgres.sh
				postgres=> \l
				postgres=> \c chainlinkropsten;
				chainlinkropsten=> \dt
				chainlinkropsten=> truncate heads cascade;
				chainlinkropsten=> truncate job_runs cascade;

	Login successful. Note that it appears that we cannot connect to the ropsten chainlink explorer, so comment this out of the ".env" file. When starting the node, the existing configuration requests the ETHUSD price hourly to the cronConsumerContract.

	Recall:
	{
		OracleAddress 			: 0x38868083fb89b571c53d2765d453d2f90f51e196
		OracleContract 			: 0x5471030a14ea46a32f065ac226516723b429ec2b
		cronConsumerContract 	: 0x187b0774af793cf6d92735e809b7457fb355fea6
		ContractCreator 		: 0xebb230E499c1cB6883Bf6996576fFe9bd4D42fAc
	}


C) Verify consumerContract interaction via an external adapter.

	Now we want to test interaction through our cryptocompare external adapter. Note that the cryptocompare external adapter has been previously deployed via AWS Lambda.

	Recall: 
	{
		ccConsumerContract		: 0xb4302F5bFf79fEbe8e75a8fcB1A62C640A798271

	}

	Perform request: This requests jobId = 1060a4c63b62495b959968863553bd37, which fetches the price of LINK. The price of LINK is stored in contact variable "uint256 public data".


		npx truffle exec scripts/externalAdapter/request-externalAdapter.js --network ropsten
		Using network 'ropsten'.

		Creating request on contract: 0xb4302F5bFf79fEbe8e75a8fcB1A62C640A798271
		{
			  specId: '0x3130363061346336336236323439356239353939363838363335353362643337',
			  requester: '0xb4302F5bFf79fEbe8e75a8fcB1A62C640A798271',
			  requestId: '0x7b1863904a0d6c70e869659241d055bfa3ae23b99454d3ac13fb52ff6104f4c5',
			  payment: '0x016345785d8a0000',
			  callbackAddr: '0xb4302F5bFf79fEbe8e75a8fcB1A62C640A798271',
			  callbackFunc: '0x4357855e',
			  expiration: '0x603ea4f7',
			  data: <Buffer bf ff>,
			  dataVersion: 1,
			  topic: '0xd8d7ecc4800d25fa53ce0372f13a416d98907a7ef3d8d3bdd79cf4fe75529c65',
			  txHash: '0x75c74190fa2cdd76f5611cc143f9f4f69ed899f08b0d25fb67341ce2075d3e6b'
		}

	Verify result: next step is to read the value written to the consumer contract by the chainlink node(=27.68$)

		npx truffle exec scripts/externalAdapter/read-externalAdapter.js --network ropsten
		Using network 'ropsten'.

		2768000000
		Truffle v5.1.14 (core: 5.1.14)
		Node v12.20.0


D) Decentralizing of data sources

	In order to perform meaningful aggregation, it is necessary to implement multiple external adapters. In this secion, multiple external adapters are deployed in AWS in order to obtain the LINKUSD price. This is the first step towards preparing a viable node image which can be used to simulate on-chain aggregation from multiple data sources. 

	In my original node configuratino, only the cryptocompare adapter was deployed. This was done via AWS lambda and a configured API endpoint:

		(old) https://8ohf3vjoo0.execute-api.eu-central-1.amazonaws.com/adapters/cl-cc

	To enable aggregation, we add some additional external adapaters to our node (bridges)	

		{
			coinGecko		: https://8ohf3vjoo0.execute-api.eu-central-1.amazonaws.com/adapters/coingecko-ea
			coinPaprika 	: https://8ohf3vjoo0.execute-api.eu-central-1.amazonaws.com/adapters/coinpaprika-ea
			cryptoCompare 	: https://8ohf3vjoo0.execute-api.eu-central-1.amazonaws.com/adapters/cryptocompare-ea
		}

	The API endpoints are verified in AWS, and jobSpecs are prepared in the chainlink node. 

		{
			coinGecko		: d38de8b842d045c28475cf573c7dfe33
			coinPaprika 	: eb731064e4e44b18a7613bbc99d60145
			cryptoCompare 	: 3b9831dba5d54f23a5f2bcbfe4e88414
		}

	Each adapter is then verified independently by calling a corresponding consumer contracts. It is important to verify that each data source is functional, prior deploying a proof of concept aggregator. 
	
		{ 
			coinGecko 		: 0xb27a109dbd57e006b33c2f88f74be5169a8b0deb4c43a811d6f89fc10cf5d1f9
			coinPaprika 	: 0x68646c5a36cb37bf6d87bb0599591f2c5f679e91c158b113269b2491a47792ae
			cryptoCompare 	: 0xbfe7a55b9395676daff5049577849ba053769b74caba03f5c210a82e1d2ecdff
		}


	Note on Copy Adapter: The old and new version of cryptocompare result was retuned under data.USD. However, when porting the jobSpec for coinGecko and coinPaprika, job runs failed at <Multiply> with message:

		"cannot parse into big.Float: : can't convert to decimal:

	Comparing runs between adapters I noticed that, despite succeeding, <Copy> was returning "null" because the jobSpec was requesting data.USD on the returned JSON (which does not exist for those adapters). When "null" is passed, it cannot be parsed big.Float and then <Multiply> fails. In this case, the jobSpec <Copy> adapter must look for data.result for each respective job run. 


E) Decentralizing the network 

	In this step, the objective is to generalize our node architecture to accommodate the deployment of a decentralized network of chainlink nodes (n=3). The nodes will be implemented as a single AWS EC2 instance running separate docker containers for each node.

	E.1) Configuring AWS

		 It is useful to prepare a separate environment, so first we need to backup the current node environemnt in case we break something: 

			a) Create snapshot of RDS instance (for backup)
				- "chainlink-ropsten-030321"

			b) Create disk image of current setup "chainink-ropsten". 
				- This is the root of our new AMI fork "chainlink-flux-monitor"
			
			c) Create new "flux-monitor" EC2 instnce 
				- Create new pem key for the instance (chainlink-flux-monitor.pem)
				- Create new elasticIP and associate with instance (3.121.16.220)
				- Add chainlink-logging-role to instance for cloud watch (Security > Modify IAM role) 
				- Add pem key, elastic IP to local scripts for mounting remote node (ssh,sshfs)
				- Verify the the current configuration works on the new instance image

			d) Back up external adapter configurations 
				- jobSpecs for each implemented external adapter (for unit testing)
				- bridge configurations for each adapter 

			e) Create a separate git-repository for the fluxMonitor work
				- Avoid confusion when working with scripts
			
	E.2) Developing chainlinkDeployer	

		After doing the above, we have a safe environment to work in. Next is to generalize our instance by running more than one node. The original fully-implemented and unit tested single node instance node was developed using a series of shell scripts. However, after working on the problem, it was decided to migrate the server side to python. This allows for better organization of data, greater automation, modular code, and symbolic deployment of nodes on the netowrk. 

		Before proceeding, we need to create a new database for each node in our RDS instance.

			postgres=> CREATE ROLE mesoic WITH PASSWORD 'password' CREATEDB CREATEROLE LOGIN;
			postgres=> GRANT rds_superuser TO mesoic;

		We want to clone the master user. This allows us to perform DB maniuplations and connect remotely without accidentally interacting with old configurations. Now we create some new tables. 

			postgres=> create database fluxnode0;
			postgres=> create database fluxnode1;
			postgres=> create database fluxnode2;

		Check the databases created (postgres=> \l)

			fluxnode0     | mesoic       | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
			fluxnode1     | mesoic       | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 
			fluxnode2     | mesoic       | UTF8     | en_US.UTF-8 | en_US.UTF-8 | 

		Now we want to create a new node which points to each database. In order to do this, the node infrastructure was rewritten to facilitate quasi-automated deployment of nodes.

			https://github.com/mesoic/chainlinkDeployer

		The script generates of ".env" files and takes care of invoking the docker containers correctly (both on firstrun, and on ordinary run). After developing chainlinkDeployer, is is straightforward to generate a network of chainlink nodes as shown in "run-fluxmonitor.py": 

			CONTAINER ID        IMAGE                     COMMAND                  CREATED             STATUS              PORTS                    NAMES
			3df22d101a67        smartcontract/chainlink   "chainlink local node"   14 minutes ago      Up 12 minutes       0.0.0.0:6688->6688/tcp   fluxNode2
			4c66bfea8393        smartcontract/chainlink   "chainlink local node"   14 minutes ago      Up 14 minutes       0.0.0.0:6687->6688/tcp   fluxNode1
			b41e47082193        smartcontract/chainlink   "chainlink local node"   15 minutes ago      Up 14 minutes       0.0.0.0:6686->6688/tcp   fluxNode0
			c629898694bb        fiews/cl-eth-failover     "node index.js wss:/���"   15 minutes ago      Up 15 minutes       0.0.0.0:4000->4000/tcp   eaas-failover
						
		The network consists of 3 fully independent nodes, looking at common EaaS provider. In this step it is important to do the port forwarding correctly and to verify the address of each node on its respective port. With some ssh tunneling, it is possible to have the GUI each node in a separate tab in the browser so we can monitor job requests. 

	E.3) On-chain configuration	and testing

		After getting the instances running and configuring the networking, the final step is to deploy oracle contracts for each node. For each node, we deploy an oracle contract (Oracle.sol), and we point the contract to our node by calling "setFulfillmentPermission(address, true)". Note we could also transferOwnership of these oracle contracts to each respective node if desired. The point is to model the idea of three fully independent node operators as accurately as possible. 

		Using remix, we deploy three copies of Oracle.sol, and run setFulfillmentPermission(_address, true) on each contrct. After doing the transactions, we can confirm permissions have been set via getAuthorizationStatus(_address). 

			{ 
				"fluxNode0" : { 
					"_address"	: "0x0296FB97aABe4550120EE5340d38f5FA8FA05d72",
					"_oracle"	: "0x9467b20a09122741c93d59808752f2e5559051cc"
				},
			
				"fluxNode1" : { 
					"_address"	: "0xbBEEf2F01bD0F3Eee0944bC46Db583Cf1db6C541",
					"_oracle"	: "0x3a5c66eb85290ad6b1a76baf30bc51805f3d2ab8"
				},

				"fluxNode2" : { 
					"_address"	: "0x3Cf7c9273b5fA06eB9E7b72D5FB4f6A349e898b3",
					"_oracle"	: "0x88d18c7436e21ffe862a73283e21e88676de197b"
				}
			}

		The final step is to add these oracle contract adresses to our configruation files in the chainlinkDeployer framework and restart the nodes. The oracle contract addresses should now be configured in the GUI.

			[ec2-user@ip-172-31-45-177 scripts]$ python stop-fluxmonitor.py
			[ec2-user@ip-172-31-45-177 scripts]$ python generate-env.py
			[ec2-user@ip-172-31-45-177 scripts]$ python start-fluxmonitor.py

		Additionally, it is necessary to fund the each node with some Ropsten ETH prior to testing external adapters. Note the on-chain steps can be seen at the following address:

			https://ropsten.etherscan.io/address/0xebb230e499c1cb6883bf6996576ffe9bd4d42fac


	E.4) Testing the nodes and deploying external adapters

		The final step in configuring our network is to add the external adapter configured in section (D) as bridges in each node and create job runs which test the functionality of adapter. A jobSpec is prepared to query the LINKUSD price through each adapter on all three nodes. 

			fluxNode0 : {
				coingecko-ea 		: 4c9502879677459681c32698add72105
				coinpaprika-ea 		: 8f3bc4026c364ba6987889dc7e3fd2a4
				cryptocompare-ea 	: ae5790c791964219a2a37bba62bdb884
			} 

			fluxNode1 : {
				coingecko-ea 		: bf9df7ccca874ec9926ef2b9726bff68
				coinpaprika-ea 		: f8cd7b2a32b0456dac623a610a15232e
				cryptocompare-ea 	: ff097e6713264beeb20d9ae5357b213b
			}


			fluxNode2 : {
				coingecko-ea 		: 86d054f982ca4b8ebac6dc65ecb1edc3
				coinpaprika-ea 		: 4c044f15f0af4322925c3800aa401ff8 
				cryptocompare-ea 	: d4f303ea587d4c338e1dd616ad32cda0
			}

			Note the following (occasinal) error on Ropsten
				TxHash 0x08ebc94f93eb54e42fb882927b4d711019f8918da705ab3e3f625c513872beda initiating 
				run ee038abf23b3446a84316d7b0b5c7828 not on main chain; presumably has been uncled
			
		Testing complete: 3(3) extenral adapters verified operational on 3(3) nodes.  

		All tests performed on consumer contract deployed at:	
			https://ropsten.etherscan.io/address/0xb4302F5bFf79fEbe8e75a8fcB1A62C640A798271


	E.5) Finalize this stage

		At this stage, all pieces are in place for implementing a minimum working example of a fluxMonitor price feed. Before continuing with fluxMonitor implementation, it is useful to back up all work:
			- Create AMI of EC2 instance 
			- Create Snapshot of RDS database
			- Push updated repositories to git
				
		












F) Running the flux monitor 

	Now that we have a node that has some working external adapters. It is possible to create a flux monitor for a given piece of price data. For this it is useful to use the following article as a basis for reference material. 

		Flux Aggregator Contracts: The Technical Explanation
			https://news.reputation.link/article/27

	Understanding how the fluxmonitor works guides the workflow. In order to set up a fluxmonitor, we need to have 

		1) multiple functional independent nodes 
			a) nodes are to be run in concurrent docker containers on the same EC2 instance
			b) each node needs a separate database 
			c) each node needs a designated address 
			d) each node needs a on-chain oracle contract

		2) an AccessControlledAggregator.sol on-chain which is configured to work with our nodes. 

G) Deployment strategy and Reflection: 

	It has been a long time since I ran a node, and a lot of things have changed since I have really been able to sit down and take a hard look at chainlink. In the meantime, I learned Solidity well and vastly improved the the Javascript. I am a lot more comfortable with the consumer contract lifecycle and basic interaction with the node. Still, I feel that my infrastructure is ugly, and there is a lot of room for imporvement. It has gotten better though on the few short bursts I spent maintaining it this past year. I don't have to work very hard to stop and start the node now in AWS, though getting live in the cloud after months of downtime is difficult because you forget the protocol after a while. I never remember what happend in many physics projects I wrote - there was weird code in there that I am sure made perfect sense when I wrote it. After all, the physics came out right. The point is, when you lose the flow (thought process) of your project, the thought process becomeshard to get ahold of again. However, you do not forget, it comes together if you work at it. Still, there is the anxiety, I feel quite far behind on the technical details of Chainlink, but I get the overall pictre.

	Keep in mind the testnet LINK, it is still valuable at the moment because the LINK faucet for Ropsten seems broken. Reduce costs to run the nodes. Let's see if we can get the flux aggregator fluxing for a long time. It would be good to flux on a consistent basis, basically to deploy a price feed that will run for a while, as long as I log in once in a while and maintain it (update the node, maybe patch the database). Soon it will be real LINK, so it is good to get in the habit of being careful. Good chainlink infrastructure is important.

	Having a maintianable testnet infrastructure is valuable in AWS because it is always working. I could not imagine doing this locally. It is just so much better to deply on some infrastructure that will last. Being able to effectively wield AWS to run a node is a strong capability. I find that most of this day was patching the AWS. Fixing the adapters, running the basic node again and verifying the consumer contracts. A node is a flexible entity with a temporal component, so you need it in the cloud to catch the low frequency. Does it really work anon? Does it really flux the price consistently? Let's see.  	

	Being able to deploy the adapters makes this task so much easier, because now (maybe) it is a matter of just deploying some nodes, deploying the contracts, configuring the contracts to point to the nodes, and hit go. The nodes watch the price and update the contract when what they see goes out of sync with the contract state. That is the flux idea. There is just a layer on on-chain reporting that happens in the protocol. It was good to move that off chain and just store the result (by the way), That's the benefit of off chain reporting. Just do the data consensus off chain and use the blockchain to store the answer. Of course, that is hard to make secure which is why it took so long.

	Deploy the fluxMonitor contract:
		AccessControlledAggregator.sol

	Contract is available here: 
		https://github.com/smartcontractkit/chainlink/blob/develop/evm-contracts/src/v0.6/AccessControlledAggregator.sol	

	Example contract (LINKETH) is here (mainnet): 
		https://etherscan.io/address/0x7E6C635d6A53B5033D1B0ceE84ecCeA9096859e4#code
		 

Flux Monitor Implementation: The objective of this exercise is to create a minimal working example of the chainlink network by launhing a cluster of nodes (n=3) and aggregating results of the API requests in in a flux aggregator contract. The flux monitor contract to be used is: 

	AccessControlledAggregator.sol

In order to achieve this, an existing node infrastructure will be used. The first step is to update the node infrastructure to the most recent version of chainlink (v0.10.1), and to verify the functionality of the node via liveness jobs(cron) which request the ETHUSD price, and via requests through an existing consumer contract which requests the LINKUSD price through the cryptocompare external adapter. All infrastructure is depolyed on the Ropsten testnet. 

Once the node is verified as functional, the existing infrastrustucture is expanded to the case of multiple nodes. For simplicity, all nodes will be deployed as separate docker containers on a single EC2 instance. 
